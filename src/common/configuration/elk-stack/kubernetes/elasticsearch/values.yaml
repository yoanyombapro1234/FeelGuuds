image:
  es:
    repository: quay.io/pires/docker-elasticsearch-kubernetes
    tag: 6.2.3
    pullPolicy: Always
  init:
    repository: busybox
    tag: latest
    pullPolicy: IfNotPresent
  curator:
    repository: bobrik/curator
    tag: latest
    pullPolicy: IfNotPresent

common:
  # Defines the service type for all outward-facing (non-discovery) services.
  # For minikube use NodePort otherwise use LoadBalancer
  serviceType: LoadBalancer

  env:
    CLUSTER_NAME: "myesdb"

    # Uncomment this if you get the "No up-and-running site-local (private)
    # addresses" error.
    # NETWORK_HOST: "_eth0_"


# Data nodes hold the shards that contain the documents you have indexed. Data
# nodes handle data related operations like CRUD, search, and aggregations.
# These operations are I/O-, memory-, and CPU-intensive. It is important to
# monitor these resources and to add more data nodes if they are overloaded.
#
# The main benefit of having dedicated data nodes is the separation of the
# master and data roles.
data:

  # This count will depend on your data and computation needs.
  replicas: 2

  env:
    NODE_DATA: "true"
    NODE_MASTER: "false"
    NODE_INGEST: "false"
    HTTP_ENABLE: "true"
    NETWORK_HOST: "0.0.0.0"

  # Determines the properties of the persistent volume claim associated with a
  # data node StatefulSet that is created when the data.stateful.enabled
  # attribute is true.
  stateful:
    enabled: false
    # This is a default value, and will not be sufficient in a production
    # system. You'll probably want to increase it.
    size: 12Gi

  # NOTE: this should be "required" in prod envs!
  podAntiAffinity: "preferred"

  customAffinity:
    # NOTE: this should be used in prod envs!
    # These pods can be scheduled ONLY on nodes with the following labels
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #     - matchExpressions:
    #       - key: role
    #         operator: In
    #         values:
    #         - elk-elasticsearch-data-node



  # The amount of RAM allocated to the JVM heap. This should be set to the
  # same value as data.resources.requests.memory, or you may see
  # OutOfMemoryErrors on startup.
  # Read: https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html
  heapMemory: 256m

  resources:
    requests:
      memory: 256Mi


# The master node is responsible for lightweight cluster-wide actions such as
# creating or deleting an index, tracking which nodes are part of the
# cluster, and deciding which shards to allocate to which nodes. It is
# important for cluster health to have a stable master node.
master:

  # Master replica count should be (#clients / 2) + 1, and generally at least 3.
  replicas: 3

  env:
    NODE_DATA: "false"
    NODE_MASTER: "true"
    NODE_INGEST: "false"
    HTTP_ENABLE: "false"
    # The default value for this environment variable is 2, meaning a cluster
    # will need a minimum of 2 master nodes to operate. If you have 3 masters
    # and one dies, the cluster still works.
    NUMBER_OF_MASTERS: "2"

  # Determines the properties of the persistent volume claim associated with a
  # data node StatefulSet that is created when the master.stateful.enabled
  # attribute is true.
  stateful:
    enabled: false
    # This is a default value, and will not be sufficient in a production
    # system. You'll probably want to increase it.
    size: 2Gi

  # NOTE: this should be "required" in prod envs!
  podAntiAffinity: "preferred"

  customAffinity:
    # NOTE: this should be used in prod envs!
    # These pods can be scheduled ONLY on nodes with the following labels
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #     - matchExpressions:
    #       - key: role
    #         operator: In
    #         values:
    #         - elk-elasticsearch-master-node

  # The amount of RAM allocated to the JVM heap. This should be set to the
  # same value as master.resources.requests.memory, or you may see
  # OutOfMemoryErrors on startup.
  # Read: https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html
  heapMemory: 256m

  resources:
    requests:
      memory: 256Mi


# Client/ingest nodes can execute pre-processing pipelines, composed of
# one or more ingest processors. Depending on the type of operations performed
# by the ingest processors and the required resources, it may make sense to
# have dedicated ingest nodes, that will only perform this specific task.
client:
  enabled: false
  # It isn't common to need more than 2 client nodes.
  replicas: 2
  podAntiAffinity: "required"

  # The amount of RAM allocated to the JVM heap. This should be set to the
  # same value as client.resources.requests.memory, or you may see
  # OutOfMemoryErrors on startup.
  heapMemory: 256m

  resources:
    requests:
      memory: 256Mi

  env:
    NODE_DATA: "false"
    NODE_MASTER: "false"
    NODE_INGEST: "true"
    HTTP_ENABLE: "true"

curator:
  enabled: true
  apiVersion: "batch/v1beta1"

  schedule: "0 1 * * *"

  # Allows modification of the default age-based filter. If you require more
  # sophisticated filtering, modify the action file specified in
  # templates/es-curator-config.yaml.
  age:
    timestring: "%Y.%m.%d"
    unit: "days"
    unit_count: 3

service:
  httpPort: 9200
  transportPort: 9300

ingress:
  enabled: false
  # hosts:
    # - chart-example.local
  # annotations:
  #   kubernetes.io/ingress.class: nginx
  #   kubernetes.io/tls-acme: "true"

exporter:
  ## number of exporter instances
  replicaCount: 1
  ## restart policy for all containers
  restartPolicy: Always
  image:
    repository: justwatch/elasticsearch_exporter
    tag: 1.0.2
    pullPolicy: IfNotPresent
  resources: {}
  service:
    type: ClusterIP
    httpPort: 9108
    annotations:
      prometheus.io/scrape: "true"
  es:
    ## Address (host and port) of the Elasticsearch node we should connect to.
    ## This could be a local node (localhost:9200, for instance), or the address
    ## of a remote Elasticsearch server. When basic auth is needed,
    ## specify as: <model>://<user>:<password>@<host>:<port>. e.g., http://admin:pass@localhost:9200.
    ## If no value, defaults to the elasticsearch service created by this Helm chart
    # uri: elasticsearch:9200

    ## If true, query stats for all nodes in the cluster, rather than just the
    ## node we connect to.
    all: true
    ## If true, query stats for all indices in the cluster.
    indices: true
    ## Timeout for trying to get stats from Elasticsearch. (ex: 20s)
    timeout: 30s
    ssl:
      ## If true, a secure connection to ES cluster is used (requires SSL certs below)
      enabled: false
      ca:
        ## PEM that contains trusted CAs used for setting up secure Elasticsearch connection
        # pem:
      client:
        ## PEM that contains the client cert to connect to Elasticsearch.
        # pem:
        ## Private key for client auth when connecting to Elasticsearch
        # key:
  web:
    ## Path under which to expose metrics.
    path: /metrics
